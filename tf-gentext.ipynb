{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brbalaji-colab/acg/blob/main/tf-gentext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
      ],
      "metadata": {
        "id": "GqCsK6Ng2Paf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frank_url = 'https://storage.googleapis.com/acg-datasets/tiny_frankenstein.tgz'\n",
        "cache_dir = '.'\n",
        "cache_subdir = 'data'\n",
        "tf.keras.utils.get_file('tiny_frankenstein.tgz', frank_url, extract=True,\n",
        "                        cache_dir=cache_dir, cache_subdir=cache_subdir)"
      ],
      "metadata": {
        "id": "OSstJ8Ui2n15",
        "outputId": "fbdb6556-fc7e-4487-9025-d20fdec418d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./data/tiny_frankenstein.tgz'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "frank_file = f'{cache_dir}/{cache_subdir}/tiny_frankenstein.txt'\n",
        "\n",
        "with open(frank_file, 'r') as f:\n",
        "    frank_data = f.read().lower()"
      ],
      "metadata": {
        "id": "2JjFHwzb2qvq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Learn tokens\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([frank_data])\n",
        "known_words = len(tokenizer.word_index)\n",
        "total_tokens = known_words + 1  # padding token"
      ],
      "metadata": {
        "id": "Qs4QWXID20mr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to tokens\n",
        "frank_tokens = tokenizer.texts_to_sequences([frank_data])[0]"
      ],
      "metadata": {
        "id": "UAWI9rhR27J6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create sequences\n",
        "def wrangle_data(sequence, examples, batch_size):\n",
        "    examples = examples + 1\n",
        "    seq_expand = tf.expand_dims(sequence, -1)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices(seq_expand)\n",
        "    dataset = dataset.window(examples, shift=1, drop_remainder=True)\n",
        "    dataset = dataset.flat_map(lambda b: b.batch(examples))\n",
        "    dataset = dataset.map(lambda x: (x[:-1], x[-1]))\n",
        "    dataset = dataset.batch(batch_size)\n",
        "    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "bsSJt2km27YH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 72\n",
        "train_data = wrangle_data(frank_tokens, seq_length, 64)"
      ],
      "metadata": {
        "id": "LFfNCdE427oK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bd_rnn(token_count, sequence_length):\n",
        "    new_model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(token_count, 32, input_length=sequence_length),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
        "        tf.keras.layers.Dense(token_count, activation='softmax')\n",
        "    ])\n",
        "    new_model.compile(optimizer=tf.keras.optimizers.Adam(0.03), loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "    return new_model"
      ],
      "metadata": {
        "id": "O0B5VQbG3Di0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = bd_rnn(total_tokens, seq_length)\n",
        "history = model.fit(train_data, epochs=10)\n",
        "\n",
        "model.save('frankenstein.h5')"
      ],
      "metadata": {
        "id": "7vGAsvc33GVH",
        "outputId": "f9acb743-ae6c-4f4d-f0d9-08ac9fc579a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "487/487 [==============================] - 25s 33ms/step - loss: 6.6129 - accuracy: 0.0764\n",
            "Epoch 2/10\n",
            "487/487 [==============================] - 9s 19ms/step - loss: 5.7179 - accuracy: 0.1097\n",
            "Epoch 3/10\n",
            "487/487 [==============================] - 11s 22ms/step - loss: 5.4687 - accuracy: 0.1202\n",
            "Epoch 4/10\n",
            "487/487 [==============================] - 10s 21ms/step - loss: 5.1797 - accuracy: 0.1302\n",
            "Epoch 5/10\n",
            "487/487 [==============================] - 10s 21ms/step - loss: 4.9129 - accuracy: 0.1442\n",
            "Epoch 6/10\n",
            "487/487 [==============================] - 13s 26ms/step - loss: 4.5980 - accuracy: 0.1618\n",
            "Epoch 7/10\n",
            "487/487 [==============================] - 10s 20ms/step - loss: 4.3192 - accuracy: 0.1848\n",
            "Epoch 8/10\n",
            "487/487 [==============================] - 10s 20ms/step - loss: 4.0371 - accuracy: 0.2132\n",
            "Epoch 9/10\n",
            "487/487 [==============================] - 10s 20ms/step - loss: 3.8081 - accuracy: 0.2344\n",
            "Epoch 10/10\n",
            "487/487 [==============================] - 12s 24ms/step - loss: 3.5867 - accuracy: 0.2606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "1f0-tXZc4Syx",
        "outputId": "06c93d3b-46b5-4369-bc48-858f0c8a7121",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jun 20 17:37:19 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P0    32W /  70W |    861MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict text\n",
        "token_lookup = {v: k for k, v in tokenizer.word_index.items()}\n",
        "\n",
        "seed = frank_tokens[-seq_length:]\n",
        "seed_text = \"\"\n",
        "\n",
        "for t in seed:\n",
        "    seed_text += token_lookup[t] + \" \"\n",
        "print(seed_text)\n",
        "\n",
        "gen_tokens = 50\n",
        "\n",
        "output = []\n",
        "\n",
        "for _ in range(gen_tokens):\n",
        "    tokens = pad_sequences([seed], maxlen=seq_length, padding='pre', truncating='pre')\n",
        "    prediction = model.predict(tokens)\n",
        "    next_token = np.argmax(prediction)\n",
        "    output.append(token_lookup[next_token + 1])\n",
        "    seed.append(next_token)\n",
        "\n",
        "print(' '.join(output))"
      ],
      "metadata": {
        "id": "ycDPoVyF4Ghj",
        "outputId": "bf188508-9e7f-46c2-b91a-5f63fc0bdbd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wickedness these motives urged me to comply with his demand we crossed the ice therefore and ascended the opposite rock the air was cold and the rain again began to descend we entered the hut the fiend with an air of exultation i with a heavy heart and depressed spirits but i consented to listen and seating myself by the fire which my odious companion had lighted he thus began his tale \n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "no and woe to striving cold her to but men valley now which be and murderer's i spirit as to but approach to would ask our sorrow many which an foresaw that and england greatest body like unhappiness had to before in secrets that and intended i pursuit your to\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}